{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sacremoses\n!pip install av\n!pip install transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import FSMTForConditionalGeneration, FSMTTokenizer, AutoImageProcessor, AutoTokenizer, VisionEncoderDecoderModel\nimport av\nimport numpy as np\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n#translation\nmname = \"facebook/wmt19-en-ru\"\ntr_tokenizer = FSMTTokenizer.from_pretrained(mname)\ntranslate = FSMTForConditionalGeneration.from_pretrained(mname).to(device)\n\n# load pretrained processor, tokenizer, and model for video processing\nimage_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"Neleac/timesformer-gpt2-video-captioning\").to(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def VideoEncode(image_processor, tokenizer, model, video_path, tr_tokenizer, translate)\n\n    try:\n        container = av.open(video_path)\n    except:\n        continue\n\n  # extract evenly spaced frames from video\n    captions = []\n    total_len = container.streams.video[0].frames\n    clip_len = model.config.encoder.num_frames\n    for i in range((total_len//30)//clip_len):\n        # print(i)\n        fp = i*30*8\n        sp = fp+240\n        indices = set(np.linspace(fp, sp, num=clip_len, endpoint=False).astype(np.int64))\n        # print(indices)\n        frames = []\n        container.seek(0)\n        for k, frame in enumerate(container.decode(video=0)):\n            if k in indices:\n                frames.append(frame.to_ndarray(format=\"rgb24\"))\n                # generate caption\n        gen_kwargs = {\n        \"min_length\": 20,\n        \"max_length\": 32,\n        \"num_beams\": 8,}\n        pixel_values = image_processor(frames, return_tensors=\"pt\").pixel_values.to(device)\n        tokens = model.generate(pixel_values, **gen_kwargs)\n        caption = tokenizer.batch_decode(tokens, skip_special_tokens=True)[0]\n        captions.append(caption)\n        video_inf = ' '.join(captions)\n        \n        #translation\n        input = video_inf\n        input_ids = tr_tokenizer.encode(input, return_tensors=\"pt\").to(device)\n        outputs = translate.generate(input_ids)\n        decoded = tr_tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        return decoded","metadata":{},"execution_count":null,"outputs":[]}]}